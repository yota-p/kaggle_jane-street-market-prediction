# Discussion: https://www.kaggle.com/c/jane-street-market-prediction/discussion/214968
# From:  https://www.kaggle.com/a763337092/pytorch-resnet-starter-training?scriptVersionId=52928216
# TODO: inplement inference: https://www.kaggle.com/a763337092/pytorch-resnet-starter-inference?scriptVersionId=52736172
import os
import sys
import time
from pathlib import Path
import random
import numpy as np
import pandas as pd
import pprint
import shutil
from typing import List, Tuple, Any
import hydra
from omegaconf.dictconfig import DictConfig
from sklearn.metrics import log_loss, roc_auc_score
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.nn import BCEWithLogitsLoss
from torch.optim.lr_scheduler import OneCycleLR
from torch.nn.modules.loss import _WeightedLoss
import torch.nn.functional as F
from src.models.torchnn import ModelV1, EarlyStopping
from src.util.get_environment import get_datadir, get_exec_env, get_device
from src.util.fast_fillna import fast_fillna
from src.util.calc_utility_score import utility_score_bincount
from src.util.calc_cross_feature import calc_cross_feature
from src.features.basetransformer import BaseTransformer


def create_janeapi() -> Tuple[Any, Any]:
    DATA_DIR = get_datadir()
    if get_exec_env() not in ['kaggle-Interactive', 'kaggle-Batch']:
        sys.path.append(f'{DATA_DIR}/raw')
    import janestreet
    env = janestreet.make_env()  # initialize the environment
    iter_test = env.iter_test()  # an iterator which loops over the test set
    return env, iter_test


def predict(models: List[Any], feature_engineering: DictConfig, target_cols: List[str], OUT_DIR: str, device, f_mean) -> None:
    env, iter_test = create_janeapi()
    feat_cols = [f'feature_{i}' for i in range(130)]
    print('Start predicting')
    time_start = time.time()
    if feature_engineering.method_fillna == 'forward':
        tmp = np.zeros(len(feat_cols))  # this np.ndarray will contain last seen values for features

    for (test_df, pred_df) in iter_test:
        if test_df['weight'].item() > 0:
            # For forward fillna, using high-performance logic by Yirun Zhang
            if feature_engineering.method_fillna == '-999':
                X_test = test_df.loc[:, feat_cols]
                X_test.fillna(-999)
                x_tt = X_test.values
            elif feature_engineering.method_fillna == 'mean':
                x_tt = test_df.loc[:, feat_cols].values
                if np.isnan(x_tt.sum()):
                    x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean
            elif feature_engineering.method_fillna == 'forward':
                x_tt = test_df.loc[:, feat_cols].values  # this is (1,130) ndarray([[values...]])
                x_tt[0, :] = fast_fillna(x_tt[0, :], tmp)  # use values in tmp to replace nan
                tmp = x_tt[0, :]  # save last seen values to tmp
            else:
                raise ValueError(f'Invalid feature_engineering.method_fillna: {feature_engineering.method_fillna}')

            if feature_engineering.cross:
                x_tt = calc_cross_feature(x_tt)

            pred = np.zeros((1, len(target_cols)))
            for model in models:
                pred += model(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / len(models)
            pred = np.median(pred)
            pred_df.action = np.where(pred >= 0.5, 1, 0).astype(int)
        else:
            pred_df.action = 0
        env.predict(pred_df)

    elapsed_time = time.time() - time_start
    test_len = 15219  # length of test data (for developing API)
    print('End predicting')
    print(f'Prediction time: {elapsed_time} s, Prediction speed: {test_len / elapsed_time} iter/s')

    # move submission file generated by env into experiment directory
    shutil.move('submission.csv', f'{OUT_DIR}/submission.csv')
    return None


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


# Model&Data fnc
class SmoothBCEwLogits(_WeightedLoss):
    def __init__(self, weight=None, reduction='mean', smoothing=0.0):
        super().__init__(weight=weight, reduction=reduction)
        self.smoothing = smoothing
        self.weight = weight
        self.reduction = reduction

    @staticmethod
    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):
        assert 0 <= smoothing < 1
        with torch.no_grad():
            targets = targets * (1.0 - smoothing) + 0.5 * smoothing
        return targets

    def forward(self, inputs, targets):
        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)
        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)

        if self.reduction == 'sum':
            loss = loss.sum()
        elif self.reduction == 'mean':
            loss = loss.mean()

        return loss


class NaFiller(BaseTransformer):
    def __init__(self, method: str, target_cols: List[str]) -> None:
        self.method_ = method
        self.target_cols = target_cols
        self.mean_: pd.DataFrame = None

    def fit(self, X):
        if self.method_ == 'mean':
            self.mean_ = X[self.target_cols].mean()
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        if self.method_ == '-999':
            X.fillna(-999, inplace=True)
        elif self.method_ == 'mean':
            X.fillna(self.mean_, inplace=True)
        elif self.method_ == 'forward':
            X.fillna(method='ffill').fillna(0, inplace=True)
        elif self.method is None:
            pass
        else:
            raise ValueError(f'Invalid method: {self.method}')
        return X


class CrossFeatureCalculator(BaseTransformer):
    def __init__(self, generate_cols: List[str]):
        self.generate_cols = generate_cols
        menu = ['cross_41_42_43', 'cross_1_2']
        if generate_cols is not None:
            abnormal = []
            for c in generate_cols:
                if c not in menu:
                    abnormal.append(c)
            if len(abnormal) > 0:
                raise ValueError(f'Invalid cols: {abnormal}')

    def transform(self, X):
        if self.generate_cols is not None:
            X['cross_41_42_43'] = X['feature_41'] + X['feature_42'] + X['feature_43']
            X['cross_1_2'] = X['feature_1'] / (X['feature_2'] + 1e-5)
        return X


class MarketDataset(Dataset):
    def __init__(self, df, all_feat_cols, target_cols):
        self.features = df[all_feat_cols].values

        self.label = df[target_cols].values.reshape(-1, len(target_cols))

    def __len__(self):
        return len(self.label)

    def __getitem__(self, idx):
        return {
            'features': torch.tensor(self.features[idx], dtype=torch.float),
            'label': torch.tensor(self.label[idx], dtype=torch.float)
        }


def get_model(
        model_name: str,
        param: DictConfig,
        feat_cols: List[str],
        target_cols: List[str],
        device: torch.device,) -> nn.Module:

    if model_name == 'torch_v1':
        model = ModelV1(feat_cols, target_cols, param.dropout_rate, param.hidden_size)
        model.to(device)
        return model
    else:
        raise ValueError(f'Invalid model: {model_name}')


def get_optimizer(
        optimizer_name: str,
        param: DictConfig,
        model_param) -> torch.optim.Optimizer:
    if optimizer_name == 'Adam':
        return torch.optim.Adam(model_param, lr=param.lr, weight_decay=param.weight_decay)
        # optimizer = Nadam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        # optimizer = Lookahead(optimizer=optimizer, k=10, alpha=0.5)
    else:
        raise ValueError(f'Invalid optimizer: {optimizer_name}')


def get_scheduler(
        scheduler_name: str,
        param: DictConfig,
        steps_per_epoch: int,
        optimizer: torch.optim.Optimizer) -> Any:
    if scheduler_name is None:
        return None
    elif scheduler_name == 'OneCycleLR':
        return OneCycleLR(
                    optimizer=param.optimizer,
                    pct_start=param.pct_start,
                    div_factor=param.dev_factor,
                    max_lr=param.max_lr,
                    epochs=param.epochs,
                    steps_per_epoch=steps_per_epoch)
    else:
        raise ValueError(f'Invalid scheduler: {scheduler_name}')


def get_loss_function(
        loss_function_name: str,
        param: DictConfig) -> torch.nn.modules.loss._Loss:  # _WeightedLoss or BCEWithLogitsLoss

    if loss_function_name == 'SmoothBCEwLogits':
        return SmoothBCEwLogits(smoothing=param.smoothing)
    elif loss_function_name == 'BCEWithLogitsLoss':
        return BCEWithLogitsLoss()
    else:
        raise ValueError(f'Invalid loss functin: {loss_function_name}')


def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):
    model.train()
    final_loss = 0

    for data in dataloader:
        optimizer.zero_grad()
        features = data['features'].to(device)
        label = data['label'].to(device)
        outputs = model(features)
        loss = loss_fn(outputs, label)
        loss.backward()
        optimizer.step()
        if scheduler:
            scheduler.step()

        final_loss += loss.item()

    final_loss /= len(dataloader)

    return final_loss


def inference_fn(model, dataloader, device, target_cols):
    model.eval()
    preds = []

    for data in dataloader:
        features = data['features'].to(device)

        with torch.no_grad():
            outputs = model(features)

        preds.append(outputs.sigmoid().detach().cpu().numpy())

    preds = np.concatenate(preds).reshape(-1, len(target_cols))

    return preds


@hydra.main(config_path='../config/train_v2', config_name='config')
def main(cfg: DictConfig) -> None:
    pprint.pprint(dict(cfg))
    DATA_DIR = get_datadir()
    OUT_DIR = f'{DATA_DIR}/{cfg.out_dir}'
    Path(OUT_DIR).mkdir(exist_ok=True, parents=True)

    seed_everything(seed=42)
    feat_cols = [f'feature_{i}' for i in range(130)]
    target_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']
    all_feat_cols = [col for col in feat_cols]

    # Feature engineering
    # https://www.kaggle.com/lucasmorin/running-algos-fe-for-fast-inference/data
    # eda: https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance
    # his example: https://www.kaggle.com/gracewan/plot-model
    print('Start feature engineering')
    # load data
    # df = pd.read_csv(f'{DATA_DIR}/raw/train.csv')
    # df.to_pickle(f'{OUT_DIR}/rawdf.pkl')
    # df = pd.read_pickle(f'{OUT_DIR}/rawdf.pkl')
    df = pd.read_pickle(f'{DATA_DIR}/processed/basic_v1/train.pkl')
    df = df[feat_cols+['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']]
    print(f'Input df.shape: {df.shape}')

    # eliminate data size for debug
    if cfg.mlflow.experiment.tags.exec == 'dev':
        print('Developing execution. Extracting records for mod(date, 10) == 0')
        df = df[np.mod(df['date'], 10) == 0]

    df = df.loc[df.date > 85].reset_index(drop=True)

    df['action'] = (df['resp'] > 0).astype('int')
    df['action_1'] = (df['resp_1'] > 0).astype('int')
    df['action_2'] = (df['resp_2'] > 0).astype('int')
    df['action_3'] = (df['resp_3'] > 0).astype('int')
    df['action_4'] = (df['resp_4'] > 0).astype('int')

    # Fill missing values
    nfl = NaFiller(cfg.feature_engineering.method_fillna, feat_cols)
    df = nfl.fit_transform(df)
    if nfl.mean_ is not None:
        f_mean = nfl.mean_.values
        np.save(f'{OUT_DIR}/nfl_mean.npy', f_mean)

    # Calculate cross features
    crs = CrossFeatureCalculator(cfg.feature_engineering.cross.cols)
    df = crs.fit_transform(df)
    if cfg.feature_engineering.cross.cols is not None:
        all_feat_cols.extend(cfg.feature_engineering.cross.cols)

    if cfg.cv.name == 'nocv':
        print('Training on full data. Note that validation data overlaps train, which will overfit!')
        train = df
        valid = df.loc[(df.date >= 450) & (df.date < 500)].reset_index(drop=True)
    else:
        train = df.loc[df.date < 450].reset_index(drop=True)
        valid = df.loc[(df.date >= 450) & (df.date < 500)].reset_index(drop=True)
    print(f'Finished feature engineering. train.shape: {train.shape}, valid.shape: {valid.shape}')

    # Train
    NMODELS = cfg.train.param.n_models
    train_set = MarketDataset(train, all_feat_cols, target_cols)
    train_loader = DataLoader(train_set, batch_size=cfg.train.param.batch_size, shuffle=True, num_workers=4)
    valid_set = MarketDataset(valid, all_feat_cols, target_cols)
    valid_loader = DataLoader(valid_set, batch_size=cfg.train.param.batch_size, shuffle=False, num_workers=4)

    if cfg.option.train:
        start_time = time.time()
        for i in range(NMODELS):
            print(f'Model{i}:')
            seed_everything(seed=42+i)

            torch.cuda.empty_cache()
            device = get_device()
            model = get_model(
                        model_name=cfg.model.name,
                        param=cfg.model.param,
                        feat_cols=all_feat_cols,
                        target_cols=target_cols,
                        device=device)

            optimizer = get_optimizer(
                            optimizer_name=cfg.train.optimizer.name,
                            param=cfg.train.optimizer.param,
                            model_param=model.parameters())

            scheduler = get_scheduler(
                            scheduler_name=cfg.train.scheduler.name,
                            param=cfg.train.scheduler.param,
                            steps_per_epoch=len(train_loader),
                            optimizer=optimizer)

            loss_fn = get_loss_function(
                        loss_function_name=cfg.train.loss_function.name,
                        param=cfg.train.loss_function.param)

            es = EarlyStopping(patience=cfg.train.param.early_stopping_rounds, mode='max')
            for epoch in range(cfg.train.param.epochs):
                train_loss = train_fn(model, optimizer, scheduler, loss_fn, train_loader, device)

                valid_pred = inference_fn(model, valid_loader, device, target_cols)
                valid_auc = roc_auc_score(valid[target_cols].values, valid_pred)
                # valid_logloss = log_loss(valid[target_cols].values, valid_pred)
                valid_pred = np.median(valid_pred, axis=1)
                valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)
                valid_u_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,
                                                       resp=valid.resp.values, action=valid_pred)
                print(f'Model{i} EPOCH:{epoch:3} \
                    train_loss={train_loss:.5f} \
                    valid_u_score={valid_u_score:.5f} \
                    valid_auc={valid_auc:.5f} \
                    time: {(time.time() - start_time) / 60:.2f} min')
                model_weights = f'{OUT_DIR}/model{i}.pth'
                es(valid_auc, model, model_path=model_weights)
                if es.early_stop:
                    print('Early stopping')
                    break
            # torch.save(model.state_dict(), model_weights)

        # Validate
        valid_pred = np.zeros((len(valid), len(target_cols)))
        for i in range(NMODELS):
            torch.cuda.empty_cache()
            device = get_device()
            model = get_model(
                        model_name=cfg.model.name,
                        param=cfg.model.param,
                        feat_cols=all_feat_cols,
                        target_cols=target_cols,
                        device=device)

            model_weights = f'{OUT_DIR}/model{i}.pth'
            model.load_state_dict(torch.load(model_weights))

            valid_pred += inference_fn(model, valid_loader, device, target_cols) / NMODELS
        auc_score = roc_auc_score(valid[target_cols].values, valid_pred)
        logloss_score = log_loss(valid[target_cols].values, valid_pred)

        valid_pred = np.median(valid_pred, axis=1)
        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)
        valid_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,
                                             resp=valid.resp.values, action=valid_pred)
        print(f'{NMODELS} models valid score: {valid_score}\tauc_score: {auc_score:.4f}\tlogloss_score:{logloss_score:.4f}')

    # Predict
    if cfg.option.predict:
        device = get_device()
        model_list = []
        for i in range(NMODELS):
            torch.cuda.empty_cache()
            model = get_model(
                        model_name=cfg.model.name,
                        param=cfg.model.param,
                        feat_cols=all_feat_cols,
                        target_cols=target_cols,
                        device=device)

            model_weights = f"{OUT_DIR}/model{i}.pth"
            model.load_state_dict(torch.load(model_weights))
            model.eval()
            model_list.append(model)

        predict(model_list, cfg.feature_engineering, target_cols, OUT_DIR, device, f_mean)

    return None


if __name__ == '__main__':
    main()
