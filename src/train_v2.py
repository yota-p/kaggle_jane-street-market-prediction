# Discussion: https://www.kaggle.com/c/jane-street-market-prediction/discussion/214968
# From:  https://www.kaggle.com/a763337092/pytorch-resnet-starter-training?scriptVersionId=52928216
# TODO: inplement inference: https://www.kaggle.com/a763337092/pytorch-resnet-starter-inference?scriptVersionId=52736172
import os
import sys
import time
from pathlib import Path
import random
import numpy as np
import pandas as pd
import pprint
import shutil
from typing import List, Tuple, Any
import hydra
from omegaconf.dictconfig import DictConfig
from sklearn.metrics import log_loss, roc_auc_score
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.nn import BCEWithLogitsLoss
from torch.optim.lr_scheduler import OneCycleLR
from torch.nn.modules.loss import _WeightedLoss
import torch.nn.functional as F
from src.util.get_environment import get_datadir, get_exec_env, get_device
from src.util.fast_fillna import fast_fillna
from src.util.calc_utility_score import utility_score_bincount


def create_janeapi() -> Tuple[Any, Any]:
    DATA_DIR = get_datadir()
    if get_exec_env() not in ['kaggle-Interactive', 'kaggle-Batch']:
        sys.path.append(f'{DATA_DIR}/raw')
    import janestreet
    env = janestreet.make_env()  # initialize the environment
    iter_test = env.iter_test()  # an iterator which loops over the test set
    return env, iter_test


def calc_cross_feature(x_tt: np.ndarray) -> np.ndarray:
    '''
    Function to caclulate cross feature.
    Input: x_tt: (1,N) ndarray. Column order should follow the dataframe created by janestreet api
    Output: y_tt: (1, N+k) ndarray. Right k rows contain calculated cross features.
    '''
    cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]
    cross_1_2 = x_tt[:, 1] / (x_tt[:, 2] + 1e-5)
    y_tt = np.concatenate((
        x_tt,
        np.array(cross_41_42_43).reshape(x_tt.shape[0], 1),
        np.array(cross_1_2).reshape(x_tt.shape[0], 1),
    ), axis=1)
    return y_tt


def predict(models: List[Any], feature_engineering: DictConfig, target_cols: List[str], OUT_DIR: str, device, f_mean) -> None:
    env, iter_test = create_janeapi()
    feat_cols = [f'feature_{i}' for i in range(130)]
    print('Start predicting')
    time_start = time.time()
    if feature_engineering.method_fillna == 'forward':
        tmp = np.zeros(len(feat_cols))  # this np.ndarray will contain last seen values for features

    for (test_df, pred_df) in iter_test:
        if test_df['weight'].item() > 0:
            # For forward fillna, using high-performance logic by Yirun Zhang
            if feature_engineering.method_fillna == '-999':
                X_test = test_df.loc[:, feat_cols]
                X_test.fillna(-999)
                x_tt = X_test.values
            elif feature_engineering.method_fillna == 'mean':
                x_tt = test_df.loc[:, feat_cols].values
                if np.isnan(x_tt.sum()):
                    x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean
            elif feature_engineering.method_fillna == 'forward':
                x_tt = test_df.loc[:, feat_cols].values  # this is (1,130) ndarray([[values...]])
                x_tt[0, :] = fast_fillna(x_tt[0, :], tmp)  # use values in tmp to replace nan
                tmp = x_tt[0, :]  # save last seen values to tmp
            else:
                raise ValueError(f'Invalid feature_engineering.method_fillna: {feature_engineering.method_fillna}')

            if feature_engineering.cross:
                x_tt = calc_cross_feature(x_tt)

            pred = np.zeros((1, len(target_cols)))
            for model in models:
                pred += model(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / len(models)
            pred = np.median(pred)
            pred_df.action = np.where(pred >= 0.5, 1, 0).astype(int)
        else:
            pred_df.action = 0
        env.predict(pred_df)

    elapsed_time = time.time() - time_start
    test_len = 15219  # length of test data (for developing API)
    print('End predicting')
    print(f'Prediction time: {elapsed_time} s, Prediction speed: {test_len / elapsed_time} iter/s')

    # move submission file generated by env into experiment directory
    shutil.move('submission.csv', f'{OUT_DIR}/submission.csv')
    return None


class EarlyStopping:
    def __init__(self, patience=7, mode='max'):
        self.patience = patience
        self.counter = 0
        self.mode = mode
        self.best_score = None
        self.early_stop = False
        if self.mode == 'min':
            self.val_score = np.Inf
        else:
            self.val_score = -np.Inf

    def __call__(self, epoch_score, model, model_path):

        if self.mode == 'min':
            score = -1.0 * epoch_score
        else:
            score = np.copy(epoch_score)

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(epoch_score, model, model_path)
        elif score < self.best_score:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(epoch_score, model, model_path)
            self.counter = 0

    def save_checkpoint(self, epoch_score, model, model_path):
        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:
            torch.save(model.state_dict(), model_path)
        self.val_score = epoch_score


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


# Model&Data fnc
class SmoothBCEwLogits(_WeightedLoss):
    def __init__(self, weight=None, reduction='mean', smoothing=0.0):
        super().__init__(weight=weight, reduction=reduction)
        self.smoothing = smoothing
        self.weight = weight
        self.reduction = reduction

    @staticmethod
    def _smooth(targets: torch.Tensor, n_labels: int, smoothing=0.0):
        assert 0 <= smoothing < 1
        with torch.no_grad():
            targets = targets * (1.0 - smoothing) + 0.5 * smoothing
        return targets

    def forward(self, inputs, targets):
        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1), self.smoothing)
        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)

        if self.reduction == 'sum':
            loss = loss.sum()
        elif self.reduction == 'mean':
            loss = loss.mean()

        return loss


class MarketDataset(Dataset):
    def __init__(self, df, all_feat_cols, target_cols):
        self.features = df[all_feat_cols].values

        self.label = df[target_cols].values.reshape(-1, len(target_cols))

    def __len__(self):
        return len(self.label)

    def __getitem__(self, idx):
        return {
            'features': torch.tensor(self.features[idx], dtype=torch.float),
            'label': torch.tensor(self.label[idx], dtype=torch.float)
        }


class Model(nn.Module):
    def __init__(self, all_feat_cols, target_cols, dropout_rate, hidden_size):
        super(Model, self).__init__()
        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))
        self.dropout0 = nn.Dropout(0.2)

        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)
        self.batch_norm1 = nn.BatchNorm1d(hidden_size)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)
        self.batch_norm2 = nn.BatchNorm1d(hidden_size)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)
        self.batch_norm3 = nn.BatchNorm1d(hidden_size)
        self.dropout3 = nn.Dropout(dropout_rate)

        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)
        self.batch_norm4 = nn.BatchNorm1d(hidden_size)
        self.dropout4 = nn.Dropout(dropout_rate)

        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))

        self.Relu = nn.ReLU(inplace=True)
        self.PReLU = nn.PReLU()
        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)
        # self.GeLU = nn.GELU()
        self.RReLU = nn.RReLU()

    def forward(self, x):
        x = self.batch_norm0(x)
        x = self.dropout0(x)

        x1 = self.dense1(x)
        x1 = self.batch_norm1(x1)
        # x = F.relu(x)
        # x = self.PReLU(x)
        x1 = self.LeakyReLU(x1)
        x1 = self.dropout1(x1)

        x = torch.cat([x, x1], 1)

        x2 = self.dense2(x)
        x2 = self.batch_norm2(x2)
        # x = F.relu(x)
        # x = self.PReLU(x)
        x2 = self.LeakyReLU(x2)
        x2 = self.dropout2(x2)

        x = torch.cat([x1, x2], 1)

        x3 = self.dense3(x)
        x3 = self.batch_norm3(x3)
        # x = F.relu(x)
        # x = self.PReLU(x)
        x3 = self.LeakyReLU(x3)
        x3 = self.dropout3(x3)

        x = torch.cat([x2, x3], 1)

        x4 = self.dense4(x)
        x4 = self.batch_norm4(x4)
        # x = F.relu(x)
        # x = self.PReLU(x)
        x4 = self.LeakyReLU(x4)
        x4 = self.dropout4(x4)

        x = torch.cat([x3, x4], 1)

        x = self.dense5(x)

        return x


def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):
    model.train()
    final_loss = 0

    for data in dataloader:
        optimizer.zero_grad()
        features = data['features'].to(device)
        label = data['label'].to(device)
        outputs = model(features)
        loss = loss_fn(outputs, label)
        loss.backward()
        optimizer.step()
        if scheduler:
            scheduler.step()

        final_loss += loss.item()

    final_loss /= len(dataloader)

    return final_loss


def inference_fn(model, dataloader, device, target_cols):
    model.eval()
    preds = []

    for data in dataloader:
        features = data['features'].to(device)

        with torch.no_grad():
            outputs = model(features)

        preds.append(outputs.sigmoid().detach().cpu().numpy())

    preds = np.concatenate(preds).reshape(-1, len(target_cols))

    return preds


@hydra.main(config_path='../config/train_v2', config_name='config')
def main(cfg: DictConfig) -> None:
    pprint.pprint(dict(cfg))
    DATA_DIR = get_datadir()
    OUT_DIR = f'{DATA_DIR}/{cfg.out_dir}'
    Path(OUT_DIR).mkdir(exist_ok=True, parents=True)

    NMODELS = cfg.train.param.n_models
    seed_everything(seed=42)
    feat_cols = [f'feature_{i}' for i in range(130)]
    target_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']

    # Feature engineering
    # load data
    train = pd.read_csv(f'{DATA_DIR}/raw/train.csv')

    # eliminate data size for debug
    if cfg.mlflow.experiment.tags.exec == 'dev':
        print('Developing execution. Extracting records for mod(date, 10)==0')
        train = train[np.mod(train['date'], 10) == 0]

    # https://www.kaggle.com/lucasmorin/running-algos-fe-for-fast-inference/data
    # eda: https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance
    # his example: https://www.kaggle.com/gracewan/plot-model
    train = train.loc[train.date > 85].reset_index(drop=True)

    train['action'] = (train['resp'] > 0).astype('int')
    train['action_1'] = (train['resp_1'] > 0).astype('int')
    train['action_2'] = (train['resp_2'] > 0).astype('int')
    train['action_3'] = (train['resp_3'] > 0).astype('int')
    train['action_4'] = (train['resp_4'] > 0).astype('int')
    valid = train.loc[(train.date >= 450) & (train.date < 500)].reset_index(drop=True)
    train = train.loc[train.date < 450].reset_index(drop=True)

    df = pd.concat([train[feat_cols], valid[feat_cols]]).reset_index(drop=True)
    f_mean = df.mean().values
    np.save(f'{OUT_DIR}/f_mean_online.npy', f_mean)

    # Fill missing values
    if cfg.feature_engineering.method_fillna == '-999':
        train.fillna(-999, inplace=True)
        valid.fillna(-999, inplace=True)
    elif cfg.feature_engineering.method_fillna == 'mean':
        train.fillna(df.mean(), inplace=True)
        valid.fillna(df.mean(), inplace=True)
    elif cfg.feature_engineering.method_fillna == 'forward':
        train.fillna(method='ffill').fillna(0, inplace=True)
        valid.fillna(method='ffill').fillna(0, inplace=True)
    elif cfg.feature_engineering.method_fillna is None:
        pass
    else:
        raise ValueError(f'Invalid method_fillna: {cfg.feature_engineering.method_fillna}')

    all_feat_cols = [col for col in feat_cols]

    if cfg.feature_engineering.cross:
        train['cross_41_42_43'] = train['feature_41'] + train['feature_42'] + train['feature_43']
        train['cross_1_2'] = train['feature_1'] / (train['feature_2'] + 1e-5)
        valid['cross_41_42_43'] = valid['feature_41'] + valid['feature_42'] + valid['feature_43']
        valid['cross_1_2'] = valid['feature_1'] / (valid['feature_2'] + 1e-5)

        all_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])

    # Train
    train_set = MarketDataset(train, all_feat_cols, target_cols)
    train_loader = DataLoader(train_set, batch_size=cfg.train.param.batch_size, shuffle=True, num_workers=4)
    valid_set = MarketDataset(valid, all_feat_cols, target_cols)
    valid_loader = DataLoader(valid_set, batch_size=cfg.train.param.batch_size, shuffle=False, num_workers=4)

    if cfg.option.train:
        start_time = time.time()
        for i in range(NMODELS):
            print(f'Fold{i}:')
            seed_everything(seed=42+i)
            torch.cuda.empty_cache()
            device = get_device()
            if cfg.model.name == 'torch_v1':
                model = Model(all_feat_cols, target_cols, cfg.model.param.dropout_rate, cfg.model.param.hidden_size)
            else:
                raise ValueError(f'Invalid model: {cfg.model.name}')
            model.to(device)
            # model = nn.DataParallel(model)

            if cfg.train.optimizer == 'torch.optim.Adam':
                optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train.param.lr, weight_decay=cfg.train.param.weight_decay)
                # optimizer = Nadam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
                # optimizer = Lookahead(optimizer=optimizer, k=10, alpha=0.5)
            else:
                raise ValueError(f'Invalid optimizer: {cfg.train.optimizer}')

            if cfg.train.scheduler is None:
                scheduler = None
            elif cfg.train.scheduler == 'torch.optim.lr_scheduler.OneCycleLR':
                scheduler = OneCycleLR(
                                optimizer=optimizer,
                                pct_start=0.1,
                                div_factor=1e3,
                                max_lr=1e-2,
                                epochs=cfg.trainparam.epochs,
                                steps_per_epoch=len(train_loader))
            else:
                raise ValueError(f'Invalid scheduler: {cfg.train.scheduler}')

            if cfg.train.loss_function == 'SmoothBCEwLogits':
                loss_fn = SmoothBCEwLogits(smoothing=0.005)
            elif cfg.train.loss_function == 'BCEWithLogitsLoss':
                loss_fn = BCEWithLogitsLoss()

            es = EarlyStopping(patience=cfg.train.param.early_stopping_rounds, mode='max')
            for epoch in range(cfg.train.param.epochs):
                train_loss = train_fn(model, optimizer, scheduler, loss_fn, train_loader, device)

                valid_pred = inference_fn(model, valid_loader, device, target_cols)
                valid_auc = roc_auc_score(valid[target_cols].values, valid_pred)
                # valid_logloss = log_loss(valid[target_cols].values, valid_pred)
                valid_pred = np.median(valid_pred, axis=1)
                valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)
                valid_u_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,
                                                       resp=valid.resp.values, action=valid_pred)
                print(f'Model{i} EPOCH:{epoch:3} \
                    train_loss={train_loss:.5f} \
                    valid_u_score={valid_u_score:.5f} \
                    valid_auc={valid_auc:.5f} \
                    time: {(time.time() - start_time) / 60:.2f} min')
                model_weights = f'{OUT_DIR}/online_model{i}.pth'
                es(valid_auc, model, model_path=model_weights)
                if es.early_stop:
                    print('Early stopping')
                    break
            # torch.save(model.state_dict(), model_weights)

        # Validate
        valid_pred = np.zeros((len(valid), len(target_cols)))
        for i in range(NMODELS):
            torch.cuda.empty_cache()
            device = get_device()
            if cfg.model.name == 'torch_v1':
                model = Model(all_feat_cols, target_cols, cfg.model.param.dropout_rate, cfg.model.param.hidden_size)
            else:
                raise ValueError(f'Invalid model: {cfg.model.name}')

            model.to(device)
            model_weights = f'{OUT_DIR}/online_model{i}.pth'
            model.load_state_dict(torch.load(model_weights))

            valid_pred += inference_fn(model, valid_loader, device, target_cols) / NMODELS
        auc_score = roc_auc_score(valid[target_cols].values, valid_pred)
        logloss_score = log_loss(valid[target_cols].values, valid_pred)

        valid_pred = np.median(valid_pred, axis=1)
        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)
        valid_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,
                                             resp=valid.resp.values, action=valid_pred)
        print(f'{NMODELS} models valid score: {valid_score}\tauc_score: {auc_score:.4f}\tlogloss_score:{logloss_score:.4f}')

    # Predict
    if cfg.option.predict:
        device = get_device()
        model_list = []
        for i in range(NMODELS):
            torch.cuda.empty_cache()
            model = Model(all_feat_cols, target_cols, cfg.model.param.dropout_rate, cfg.model.param.hidden_size)
            model.to(device)
            model_weights = f"{OUT_DIR}/online_model{i}.pth"
            model.load_state_dict(torch.load(model_weights))
            model.eval()
            model_list.append(model)

        predict(model_list, cfg.feature_engineering, target_cols, OUT_DIR, device, f_mean)

    return None


if __name__ == '__main__':
    main()
